{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:26:17.299521Z","iopub.execute_input":"2025-05-09T18:26:17.299770Z","iopub.status.idle":"2025-05-09T18:26:19.548416Z","shell.execute_reply.started":"2025-05-09T18:26:17.299744Z","shell.execute_reply":"2025-05-09T18:26:19.547581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom scipy import linalg\nimport os\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:42:05.442579Z","iopub.execute_input":"2025-05-09T19:42:05.442845Z","iopub.status.idle":"2025-05-09T19:42:05.454177Z","shell.execute_reply.started":"2025-05-09T19:42:05.442825Z","shell.execute_reply":"2025-05-09T19:42:05.453644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## HyperParameters","metadata":{}},{"cell_type":"code","source":"latent_dim = 100\nepochs = 50  \nbatch_size = 64\nlr = 0.0002\nbeta1 = 0.5\ncompute_fid = False \noutput_dir = \"/kaggle/working/\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:05.968260Z","iopub.execute_input":"2025-05-09T18:29:05.968931Z","iopub.status.idle":"2025-05-09T18:29:05.973169Z","shell.execute_reply.started":"2025-05-09T18:29:05.968906Z","shell.execute_reply":"2025-05-09T18:29:05.972411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load and preprocess CIFAR-10 dataset\n","metadata":{}},{"cell_type":"code","source":"def load_cifar10():\n    (x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n    x_train = (x_train.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1, 1]\n    return x_train\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:10.289500Z","iopub.execute_input":"2025-05-09T18:29:10.289796Z","iopub.status.idle":"2025-05-09T18:29:10.294248Z","shell.execute_reply.started":"2025-05-09T18:29:10.289774Z","shell.execute_reply":"2025-05-09T18:29:10.293500Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Generator\n","metadata":{}},{"cell_type":"code","source":"def build_generator():\n    model = models.Sequential([\n        layers.Input(shape=(latent_dim,)),\n        layers.Dense(4 * 4 * 256),\n        layers.Reshape((4, 4, 256)),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(negative_slope=0.2),\n        layers.Conv2DTranspose(128, 4, strides=2, padding='same'),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(negative_slope=0.2),\n        layers.Conv2DTranspose(64, 4, strides=2, padding='same'),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(negative_slope=0.2),\n        layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:12.407322Z","iopub.execute_input":"2025-05-09T18:29:12.407791Z","iopub.status.idle":"2025-05-09T18:29:12.412338Z","shell.execute_reply.started":"2025-05-09T18:29:12.407768Z","shell.execute_reply":"2025-05-09T18:29:12.411768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Build Discriminator\n","metadata":{}},{"cell_type":"code","source":"def build_discriminator():\n    model = models.Sequential([\n        layers.Input(shape=(32, 32, 3)),\n        layers.Conv2D(64, 4, strides=2, padding='same'),\n        layers.LeakyReLU(negative_slope=0.2),\n        layers.Dropout(0.3),\n        layers.Conv2D(128, 4, strides=2, padding='same'),\n        layers.LeakyReLU(negative_slope=0.2),\n        layers.Dropout(0.3),\n        layers.Conv2D(256, 4, strides=2, padding='same'),\n        layers.LeakyReLU(negative_slope=0.2),\n        layers.Flatten(),\n        layers.Dense(1)\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:14.433635Z","iopub.execute_input":"2025-05-09T18:29:14.434238Z","iopub.status.idle":"2025-05-09T18:29:14.439041Z","shell.execute_reply.started":"2025-05-09T18:29:14.434201Z","shell.execute_reply":"2025-05-09T18:29:14.438248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Relativistic average GAN loss\n","metadata":{}},{"cell_type":"code","source":"def relativistic_loss(real_logits, fake_logits):\n    real_diff = real_logits - tf.reduce_mean(fake_logits)\n    fake_diff = fake_logits - tf.reduce_mean(real_logits)\n    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.ones_like(real_diff), logits=real_diff))\n    fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.zeros_like(fake_diff), logits=fake_diff))\n    return real_loss, fake_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:15.935662Z","iopub.execute_input":"2025-05-09T18:29:15.936315Z","iopub.status.idle":"2025-05-09T18:29:15.940414Z","shell.execute_reply.started":"2025-05-09T18:29:15.936293Z","shell.execute_reply":"2025-05-09T18:29:15.939774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compute discriminator accuracy\n","metadata":{}},{"cell_type":"code","source":"def compute_discriminator_accuracy(real_logits, fake_logits):\n    real_preds = tf.cast(tf.sigmoid(real_logits) > 0.5, tf.float32)\n    fake_preds = tf.cast(tf.sigmoid(fake_logits) <= 0.5, tf.float32)\n    real_correct = tf.reduce_mean(tf.cast(real_preds, tf.float32))\n    fake_correct = tf.reduce_mean(tf.cast(fake_preds, tf.float32))\n    accuracy = (real_correct + fake_correct) / 2\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:17.043007Z","iopub.execute_input":"2025-05-09T18:29:17.043279Z","iopub.status.idle":"2025-05-09T18:29:17.048034Z","shell.execute_reply.started":"2025-05-09T18:29:17.043258Z","shell.execute_reply":"2025-05-09T18:29:17.047498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training \n","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(real_images, generator, discriminator, g_optimizer, d_optimizer):\n    batch_size = tf.shape(real_images)[0]\n    noise = tf.random.normal([batch_size, latent_dim])\n    \n    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n        fake_images = generator(noise, training=True)\n        real_logits = discriminator(real_images, training=True)\n        fake_logits = discriminator(fake_images, training=True)\n        \n        d_real_loss, d_fake_loss = relativistic_loss(real_logits, fake_logits)\n        d_loss = d_real_loss + d_fake_loss\n        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.ones_like(fake_logits), logits=fake_logits - tf.reduce_mean(real_logits)))\n    \n    d_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)\n    g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)\n    d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n    \n    accuracy = compute_discriminator_accuracy(real_logits, fake_logits)\n    return d_loss, g_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:18.914089Z","iopub.execute_input":"2025-05-09T18:29:18.914906Z","iopub.status.idle":"2025-05-09T18:29:18.921060Z","shell.execute_reply.started":"2025-05-09T18:29:18.914869Z","shell.execute_reply":"2025-05-09T18:29:18.920346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compute FID \n","metadata":{}},{"cell_type":"code","source":"def compute_fid(real_images, generator, num_images=2000):\n    try:\n        start_time = time.time()\n        timeout = 300 \n        inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n        \n        \n        noise = tf.random.normal([num_images, latent_dim])\n        fake_images = generator(noise, training=False)\n        \n       \n        real_images = tf.image.resize(real_images[:num_images], [299, 299])\n        fake_images = tf.image.resize(fake_images, [299, 299])\n        real_images = preprocess_input(real_images * 127.5 + 127.5)\n        fake_images = preprocess_input(fake_images * 127.5 + 127.5)\n        \n       \n        real_activations = inception_model.predict(real_images, batch_size=200)\n        fake_activations = inception_model.predict(fake_images, batch_size=200)\n        \n      \n        mu_real = np.mean(real_activations, axis=0)\n        mu_fake = np.mean(fake_activations, axis=0)\n        sigma_real = np.cov(real_activations, rowvar=False)\n        sigma_fake = np.cov(fake_activations, rowvar=False)\n        \n       \n        sigma_real += np.eye(sigma_real.shape[0]) * 1e-6\n        sigma_fake += np.eye(sigma_fake.shape[0]) * 1e-6\n        \n       \n        diff = mu_real - mu_fake\n        covmean = linalg.sqrtm(sigma_real.dot(sigma_fake), disp=False)[0]\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n        fid_score = diff.dot(diff) + np.trace(sigma_real + sigma_fake - 2 * covmean)\n        \n        if time.time() - start_time > timeout:\n            print(\"FID computation timed out\")\n            return None\n        \n        return max(fid_score, 0.0)\n    except Exception as e:\n        print(f\"FID computation failed: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:20.592502Z","iopub.execute_input":"2025-05-09T18:29:20.593170Z","iopub.status.idle":"2025-05-09T18:29:20.600368Z","shell.execute_reply.started":"2025-05-09T18:29:20.593145Z","shell.execute_reply":"2025-05-09T18:29:20.599831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the GAN\n","metadata":{}},{"cell_type":"code","source":"def train_gan(dataset):\n    generator = build_generator()\n    discriminator = build_discriminator()\n    g_optimizer = tf.keras.optimizers.Adam(lr, beta_1=beta1)\n    d_optimizer = tf.keras.optimizers.Adam(lr, beta_1=beta1)\n    \n    dataset_tf = tf.data.Dataset.from_tensor_slices(dataset).shuffle(60000).batch(batch_size)\n    \n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        epoch_accuracy = 0.0\n        steps = 0\n        for step, real_images in enumerate(dataset_tf):\n            d_loss, g_loss, accuracy = train_step(real_images, generator, discriminator, g_optimizer, d_optimizer)\n            if step % 100 == 0:\n                print(f\"Step {step}, D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}, D Accuracy: {accuracy:.4f}\")\n            epoch_accuracy += accuracy\n            steps += 1\n        \n        print(f\"Epoch {epoch + 1} Average D Accuracy: {epoch_accuracy / steps:.4f}\")\n        \n        noise = tf.random.normal([16, latent_dim])\n        fake_images = generator(noise, training=False)\n        plt.figure(figsize=(4, 4))\n        for i in range(16):\n            plt.subplot(4, 4, i + 1)\n            plt.imshow((fake_images[i] * 127.5 + 127.5).numpy().astype(np.uint8))\n            plt.axis('off')\n        plt.savefig(os.path.join(output_dir, f\"output_epoch_{epoch}.png\"))\n        plt.close()\n        \n        if (epoch + 1) % 10 == 0:\n            g_checkpoint_path = os.path.join(output_dir, f\"generator_epoch_{epoch + 1}.keras\")\n            d_checkpoint_path = os.path.join(output_dir, f\"discriminator_epoch_{epoch + 1}.keras\")\n            generator.save(g_checkpoint_path, save_format=\"keras_v3\")\n            discriminator.save(d_checkpoint_path, save_format=\"keras_v3\")\n            print(f\"Saved generator checkpoint: {g_checkpoint_path} (trained for {epoch + 1} epochs)\")\n            print(f\"Saved discriminator checkpoint: {d_checkpoint_path} (trained for {epoch + 1} epochs)\")\n    \n    final_g_model_path = os.path.join(output_dir, \"generator.keras\")\n    final_d_model_path = os.path.join(output_dir, \"discriminator.keras\")\n    generator.save(final_g_model_path, save_format=\"keras_v3\")\n    discriminator.save(final_d_model_path, save_format=\"keras_v3\")\n    print(f\"Saved final generator model: {final_g_model_path} (trained for {epochs} epochs)\")\n    print(f\"Saved final discriminator model: {final_d_model_path} (trained for {epochs} epochs)\")\n    \n    if epochs % 10 != 0:\n        g_checkpoint_path = os.path.join(output_dir, f\"generator_epoch_{epochs}.keras\")\n        d_checkpoint_path = os.path.join(output_dir, f\"discriminator_epoch_{epochs}.keras\")\n        generator.save(g_checkpoint_path, save_format=\"keras_v3\")\n        discriminator.save(d_checkpoint_path, save_format=\"keras_v3\")\n        print(f\"Saved final generator checkpoint: {g_checkpoint_path} (trained for {epochs} epochs)\")\n        print(f\"Saved final discriminator checkpoint: {d_checkpoint_path} (trained for {epochs} epochs)\")\n    \n    if compute_fid:\n        print(\"Computing FID...\")\n        fid_score = compute_fid(dataset, generator)\n        if fid_score is not None:\n            print(f\"FID Score: {fid_score:.2f}\")\n        else:\n            print(\"FID computation skipped due to error or timeout\")\n    \n    print(\"\\nTo download models and images:\")\n    print(\"1. Go to the 'Output' tab (right sidebar).\")\n    print(\"2. Navigate to /kaggle/working/.\")\n    print(\"3. Download 'generator.keras', 'discriminator.keras', and any 'generator_epoch_X.keras' or 'discriminator_epoch_X.keras' files.\")\n    print(\"Alternatively, create a Kaggle dataset:\")\n    print(\"Run the following in a new cell after training:\")\n    print(\"```bash\")\n    print(\"!mkdir -p /kaggle/working/gan-models\")\n    print(\"!cp /kaggle/working/*.keras /kaggle/working/gan-models/\")\n    print(\"!cp /kaggle/working/*.png /kaggle/working/gan-models/\")\n    print(\"!kaggle datasets create -p /kaggle/working/gan-models -u -r zip\")\n    print(\"```\")\n    print(\"Then download the dataset from your Kaggle profile.\")\n    \n    return generator, discriminator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:22.394649Z","iopub.execute_input":"2025-05-09T18:29:22.394906Z","iopub.status.idle":"2025-05-09T18:29:22.406163Z","shell.execute_reply.started":"2025-05-09T18:29:22.394889Z","shell.execute_reply":"2025-05-09T18:29:22.405414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main execution\n","metadata":{}},{"cell_type":"code","source":"\nx_train = load_cifar10()\ngenerator, discriminator = train_gan(x_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:29:25.754185Z","iopub.execute_input":"2025-05-09T18:29:25.754521Z","iopub.status.idle":"2025-05-09T18:49:34.407597Z","shell.execute_reply.started":"2025-05-09T18:29:25.754493Z","shell.execute_reply":"2025-05-09T18:49:34.406775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}